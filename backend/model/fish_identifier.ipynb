{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca981c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import _model as model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2afbb11-6e05-48c3-a011-e5e1439a7285",
   "metadata": {},
   "source": [
    "### Load training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d473c4-9c14-419a-8a9d-e66e9fed8951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1481 files belonging to 20 classes.\n",
      "Found 361 files belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = keras.utils.image_dataset_from_directory(\n",
    "    \"./fish_train_data\",\n",
    "    label_mode=\"int\",\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=64,\n",
    "    image_size=(512, 512),\n",
    "    shuffle=True\n",
    ")\n",
    "test_dataset = keras.utils.image_dataset_from_directory(\n",
    "    \"./fish_test_data\",\n",
    "    label_mode=\"int\",\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=64,\n",
    "    image_size=(512, 512),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f58b275-bf4b-43ed-8364-126530c1075d",
   "metadata": {},
   "source": [
    "### Display one image from the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e063db8b-0792-4df9-8003-f2d1b220c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs in train_dataset.take(1):\n",
    "    imgs, labels = inputs\n",
    "\n",
    "plt.imshow(tf.cast(imgs[2], tf.dtypes.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2bd59c-a502-4bc4-82f2-825bdf3699bb",
   "metadata": {},
   "source": [
    "### Create augmentation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8c6cc8a-6b1d-4f29-9795-ac5bb4239cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_layers = model.make_aug(fill_value=127.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9465995a-4a28-4d5e-bea6-2befe6d3a9d6",
   "metadata": {},
   "source": [
    "### Show the same image from earlier, but augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad497d46-d06c-4c47-90c8-bbb439afc505",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    tf.cast(augmentation_layers(tf.expand_dims(imgs[2], 0), training=True)[0], tf.dtypes.int32)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b26ce2-dec7-4592-8f6f-9fd353b3a3ab",
   "metadata": {},
   "source": [
    "### Training only the top dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfd44d13-0abc-4571-8cdb-f4d929cb1ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "24/24 [==============================] - 18s 315ms/step - loss: 2.7405 - accuracy: 0.1722\n",
      "Epoch 2/50\n",
      "24/24 [==============================] - 8s 289ms/step - loss: 1.6353 - accuracy: 0.5469\n",
      "Epoch 3/50\n",
      "24/24 [==============================] - 8s 285ms/step - loss: 1.0245 - accuracy: 0.6806\n",
      "Epoch 4/50\n",
      "24/24 [==============================] - 8s 288ms/step - loss: 0.9826 - accuracy: 0.6860\n",
      "Epoch 5/50\n",
      "24/24 [==============================] - 10s 378ms/step - loss: 0.8470 - accuracy: 0.7265 - val_loss: 0.6174 - val_accuracy: 0.8061\n",
      "Epoch 6/50\n",
      "24/24 [==============================] - 8s 288ms/step - loss: 0.9776 - accuracy: 0.6921\n",
      "Epoch 7/50\n",
      "24/24 [==============================] - 8s 286ms/step - loss: 0.8518 - accuracy: 0.7353\n",
      "Epoch 8/50\n",
      "24/24 [==============================] - 8s 289ms/step - loss: 0.8458 - accuracy: 0.7400\n",
      "Epoch 9/50\n",
      "24/24 [==============================] - 8s 288ms/step - loss: 0.6898 - accuracy: 0.7927\n",
      "Epoch 10/50\n",
      "24/24 [==============================] - 9s 334ms/step - loss: 0.5160 - accuracy: 0.8433 - val_loss: 0.5689 - val_accuracy: 0.8144\n",
      "Epoch 11/50\n",
      "24/24 [==============================] - 8s 289ms/step - loss: 0.6460 - accuracy: 0.7907\n",
      "Epoch 12/50\n",
      "24/24 [==============================] - 8s 286ms/step - loss: 0.6363 - accuracy: 0.7947\n",
      "Epoch 13/50\n",
      "24/24 [==============================] - 8s 289ms/step - loss: 0.5424 - accuracy: 0.8238\n",
      "Epoch 14/50\n",
      "24/24 [==============================] - 8s 288ms/step - loss: 0.5504 - accuracy: 0.8278\n",
      "Epoch 15/50\n",
      "24/24 [==============================] - 9s 333ms/step - loss: 0.4580 - accuracy: 0.8501 - val_loss: 0.4892 - val_accuracy: 0.8504\n",
      "Epoch 16/50\n",
      "24/24 [==============================] - 8s 286ms/step - loss: 0.4150 - accuracy: 0.8724\n",
      "Epoch 17/50\n",
      "24/24 [==============================] - 8s 288ms/step - loss: 0.3731 - accuracy: 0.8798\n",
      "Epoch 18/50\n",
      "24/24 [==============================] - 8s 288ms/step - loss: 0.3804 - accuracy: 0.8852\n",
      "Epoch 19/50\n",
      "24/24 [==============================] - 8s 287ms/step - loss: 0.3586 - accuracy: 0.8893\n",
      "Epoch 20/50\n",
      "24/24 [==============================] - 9s 332ms/step - loss: 0.2832 - accuracy: 0.9102 - val_loss: 0.3143 - val_accuracy: 0.9003\n",
      "Epoch 21/50\n",
      "24/24 [==============================] - 8s 288ms/step - loss: 0.3384 - accuracy: 0.8994\n",
      "Epoch 22/50\n",
      "24/24 [==============================] - 8s 290ms/step - loss: 0.2316 - accuracy: 0.9251\n",
      "Epoch 23/50\n",
      "24/24 [==============================] - 8s 288ms/step - loss: 0.2736 - accuracy: 0.9095\n",
      "Epoch 24/50\n",
      "24/24 [==============================] - 8s 289ms/step - loss: 0.3101 - accuracy: 0.9055\n",
      "Epoch 25/50\n",
      "24/24 [==============================] - 9s 334ms/step - loss: 0.2729 - accuracy: 0.9176 - val_loss: 0.3105 - val_accuracy: 0.9003\n",
      "Epoch 26/50\n",
      "24/24 [==============================] - 8s 287ms/step - loss: 0.2464 - accuracy: 0.9210\n",
      "Epoch 27/50\n",
      "24/24 [==============================] - 8s 287ms/step - loss: 0.2587 - accuracy: 0.9149\n",
      "Epoch 28/50\n",
      "24/24 [==============================] - 8s 286ms/step - loss: 0.2862 - accuracy: 0.9082\n",
      "Epoch 29/50\n",
      "24/24 [==============================] - 8s 287ms/step - loss: 0.1721 - accuracy: 0.9419\n",
      "Epoch 30/50\n",
      "24/24 [==============================] - 9s 333ms/step - loss: 0.1499 - accuracy: 0.9568 - val_loss: 0.2823 - val_accuracy: 0.9197\n",
      "Epoch 31/50\n",
      "24/24 [==============================] - 8s 289ms/step - loss: 0.2010 - accuracy: 0.9372\n",
      "Epoch 32/50\n",
      "24/24 [==============================] - 8s 288ms/step - loss: 0.1774 - accuracy: 0.9440\n",
      "Epoch 33/50\n",
      "24/24 [==============================] - 8s 286ms/step - loss: 0.1813 - accuracy: 0.9413\n",
      "Epoch 34/50\n",
      "24/24 [==============================] - 8s 287ms/step - loss: 0.1853 - accuracy: 0.9446\n",
      "Epoch 35/50\n",
      "24/24 [==============================] - 9s 333ms/step - loss: 0.1841 - accuracy: 0.9365 - val_loss: 0.2505 - val_accuracy: 0.9169\n",
      "Epoch 36/50\n",
      "24/24 [==============================] - 8s 288ms/step - loss: 0.1659 - accuracy: 0.9554\n",
      "Epoch 37/50\n",
      "24/24 [==============================] - 8s 287ms/step - loss: 0.2223 - accuracy: 0.9311\n",
      "Epoch 38/50\n",
      "24/24 [==============================] - 8s 289ms/step - loss: 0.1659 - accuracy: 0.9527\n",
      "Epoch 39/50\n",
      "24/24 [==============================] - 8s 287ms/step - loss: 0.1972 - accuracy: 0.9392\n",
      "Epoch 40/50\n",
      "24/24 [==============================] - 9s 335ms/step - loss: 0.1704 - accuracy: 0.9500 - val_loss: 0.2463 - val_accuracy: 0.9197\n",
      "Epoch 41/50\n",
      "24/24 [==============================] - 8s 285ms/step - loss: 0.2112 - accuracy: 0.9392\n",
      "Epoch 42/50\n",
      "24/24 [==============================] - 8s 288ms/step - loss: 0.1464 - accuracy: 0.9581\n",
      "Epoch 43/50\n",
      "24/24 [==============================] - 8s 289ms/step - loss: 0.2108 - accuracy: 0.9365\n",
      "Epoch 44/50\n",
      "24/24 [==============================] - 8s 288ms/step - loss: 0.1494 - accuracy: 0.9595\n",
      "Epoch 45/50\n",
      "24/24 [==============================] - 9s 332ms/step - loss: 0.2044 - accuracy: 0.9359 - val_loss: 0.2424 - val_accuracy: 0.9141\n",
      "Epoch 46/50\n",
      "24/24 [==============================] - 8s 288ms/step - loss: 0.1948 - accuracy: 0.9379\n",
      "Epoch 47/50\n",
      "24/24 [==============================] - 8s 289ms/step - loss: 0.1866 - accuracy: 0.9386\n",
      "Epoch 48/50\n",
      "24/24 [==============================] - 8s 288ms/step - loss: 0.1791 - accuracy: 0.9500\n",
      "Epoch 49/50\n",
      "24/24 [==============================] - 8s 287ms/step - loss: 0.2262 - accuracy: 0.9298\n",
      "Epoch 50/50\n",
      "24/24 [==============================] - 9s 333ms/step - loss: 0.1745 - accuracy: 0.9480 - val_loss: 0.2347 - val_accuracy: 0.9224\n"
     ]
    }
   ],
   "source": [
    "model1 = model.make_model(n_classes=20)\n",
    "efficient_net = model1.layers[1]\n",
    "efficient_net.Trainable = False\n",
    "lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    0.03, 1000\n",
    ")\n",
    "model1.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=lr_decayed_fn, momentum=0.9),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history = model1.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=test_dataset,\n",
    "    validation_freq=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ae421-a55c-44b1-ac25-cb3cf6c02122",
   "metadata": {},
   "source": [
    "### Store top model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88ef2a33-950a-437c-9d22-17cc8f803904",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.layers[-1].save_weights(\"trained_weights/trained_top\")\n",
    "del model1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0d8b56",
   "metadata": {},
   "source": [
    "### Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72144873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f97148fec10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = model.make_model(n_classes=20)\n",
    "model2.layers[-1].load_weights(\"trained_weights/trained_top\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8928e7f",
   "metadata": {},
   "source": [
    "### Run model on locally saved image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "190c5761-f57c-46c5-866e-fd1f814fb0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0315797 , 0.01744416, 0.04716867, 0.01206983, 0.20707177,\n",
       "        0.01890964, 0.00788425, 0.00315444, 0.00250336, 0.15584128,\n",
       "        0.1785851 , 0.01913932, 0.00060353, 0.07811665, 0.01061875,\n",
       "        0.00892797, 0.00837946, 0.0144966 , 0.15275924, 0.02474626]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = keras.utils.load_img(\"salmon.jpg\", target_size=(512, 512))\n",
    "batch = np.expand_dims(keras.utils.img_to_array(img), axis=0)\n",
    "model2.predict(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ML': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "723f92442ed367d4370f31d60b5f82cb3c62acc21d19d96103bc7b04b520517d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
